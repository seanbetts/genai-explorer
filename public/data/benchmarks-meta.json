[
  {
    "benchmark_id": "browse-comp",
    "benchmark_name": "BrowseComp",
    "benchmark_category": "agentic",
    "featured_benchmark": false,
    "benchmark_description": "A Simple Yet Challenging Benchmark for Browsing Agents",
    "benchmark_paper": "https://openai.com/index/browsecomp/"
  },
  {
    "benchmark_id": "collie",
    "benchmark_name": "COLLIE",
    "benchmark_category": "agentic",
    "featured_benchmark": false,
    "benchmark_description": "A grammar-based framework that enables systematic construction of compositional constraints",
    "benchmark_paper": "https://arxiv.org/pdf/2307.08689"
  },
  {
    "benchmark_id": "complex-func-bench",
    "benchmark_name": "ComplexFuncBench",
    "benchmark_category": "agentic",
    "featured_benchmark": false,
    "benchmark_description": "A benchmark forcomplex function calling across five real-world scenarios",
    "benchmark_paper": "https://arxiv.org/pdf/2501.10132"
  },
  {
    "benchmark_id": "if-eval",
    "benchmark_name": "IFEval",
    "benchmark_category": "agentic",
    "featured_benchmark": false,
    "benchmark_description": "A straightforward and easy-to-reproduce evaluation benchmark that focuses on a set of \"verifiable instructions\"",
    "benchmark_paper": "https://arxiv.org/abs/2311.07911"
  },
  {
    "benchmark_id": "multi-if",
    "benchmark_name": "Multi-IF",
    "benchmark_category": "agentic",
    "featured_benchmark": false,
    "benchmark_description": "A benchmark designed to assess LLMs’ proficiency in following multi-turn and multilingual instructions",
    "benchmark_paper": "https://arxiv.org/html/2410.15553v2"
  },
  {
    "benchmark_id": "multi-challenge",
    "benchmark_name": "MultiChallenge",
    "benchmark_category": "agentic",
    "featured_benchmark": false,
    "benchmark_description": "A benchmark evaluating LLMs on conducting multi-turn conversations",
    "benchmark_paper": "https://scale.com/leaderboard/multichallenge"
  },
  {
    "benchmark_id": "tau-bench-airline",
    "benchmark_name": "Tau-bench Airline",
    "benchmark_category": "agentic",
    "featured_benchmark": true,
    "benchmark_description": "A framework that tests AI agents on complex real-world tasks with user and tool interactions",
    "benchmark_paper": "https://arxiv.org/pdf/2406.12045"
  },
  {
    "benchmark_id": "tau-bench-retail",
    "benchmark_name": "Tau-bench Retail",
    "benchmark_category": "agentic",
    "featured_benchmark": true,
    "benchmark_description": "A framework that tests AI agents on complex real-world tasks with user and tool interactions",
    "benchmark_paper": "https://arxiv.org/pdf/2406.12045"
  },
  {
    "benchmark_id": "aider-polyglot",
    "benchmark_name": "Aider Polyglot",
    "benchmark_category": "coding",
    "featured_benchmark": true,
    "benchmark_description": "Evaluates an LLM’s ability to follow instructions and edit code successfully without human intervention",
    "benchmark_paper": "https://aider.chat/docs/leaderboards/"
  },
  {
    "benchmark_id": "bird-sql",
    "benchmark_name": "Bird-SQL",
    "benchmark_category": "coding",
    "featured_benchmark": false,
    "benchmark_description": "Benchmark evaluating converting natural language questions into executable SOL",
    "benchmark_paper": "https://bird-bench.github.io"
  },
  {
    "benchmark_id": "codeforces",
    "benchmark_name": "Codeforces",
    "benchmark_category": "coding",
    "featured_benchmark": false,
    "benchmark_description": "A competition coding benchmark designed to accurately evaluate the reasoning capabilities of LLMs with human-comparable standardized ELO ratings",
    "benchmark_paper": "https://arxiv.org/html/2501.01257v2"
  },
  {
    "benchmark_id": "human-eval",
    "benchmark_name": "HumanEval",
    "benchmark_category": "coding",
    "featured_benchmark": false,
    "benchmark_description": "Evaluating Large Language Models Trained on Code",
    "benchmark_paper": "https://github.com/openai/human-eval"
  },
  {
    "benchmark_id": "live-code-bench",
    "benchmark_name": "LiveCodeBench",
    "benchmark_category": "coding",
    "featured_benchmark": false,
    "benchmark_description": "Code generation in Python subset covering more recent examples",
    "benchmark_paper": "https://livecodebench.github.io"
  },
  {
    "benchmark_id": "swe-bench-verified",
    "benchmark_name": "SWE-Bench Verified",
    "benchmark_category": "coding",
    "featured_benchmark": true,
    "benchmark_description": "Ability to solve real-world software issues",
    "benchmark_paper": "https://openai.com/index/introducing-swe-bench-verified/"
  },
  {
    "benchmark_id": "swe-lancer",
    "benchmark_name": "SWE-Lancer",
    "benchmark_category": "coding",
    "featured_benchmark": false,
    "benchmark_description": "A benchmark of over 1,400 freelance software engineering tasks from Upwork, valued at $1 million USD total in real-world payouts",
    "benchmark_paper": "https://openai.com/index/swe-lancer/"
  },
  {
    "benchmark_id": "swe-lancer-ic-swe-diamond",
    "benchmark_name": "SWE-Lancer: IC SWE Diamond",
    "benchmark_category": "coding",
    "featured_benchmark": false,
    "benchmark_description": "A benchmark of over 1,400 freelance software engineering tasks from Upwork, valued at $1 million USD total in real-world payouts",
    "benchmark_paper": "https://openai.com/index/swe-lancer/"
  },
  {
    "benchmark_id": "big-bench-hard",
    "benchmark_name": "BIG-Bench-Hard",
    "benchmark_category": "reasoning",
    "featured_benchmark": false,
    "benchmark_description": "A diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models",
    "benchmark_paper": "https://paperswithcode.com/dataset/bbh"
  },
  {
    "benchmark_id": "chatbot-arena",
    "benchmark_name": "Chatbot Arena",
    "benchmark_category": "General Intelligence",
    "featured_benchmark": true,
    "benchmark_description": "An open platform for crowdsourced AI benchmarking",
    "benchmark_paper": "https://lmarena.ai"
  },
  {
    "benchmark_id": "mmlu",
    "benchmark_name": "MMLU",
    "benchmark_category": "General Intelligence",
    "featured_benchmark": false,
    "benchmark_description": "Measuring Massive Multitask Language Understanding",
    "benchmark_paper": "https://github.com/hendrycks/test"
  },
  {
    "benchmark_id": "mmlu-pro",
    "benchmark_name": "MMLU-pro",
    "benchmark_category": "General Intelligence",
    "featured_benchmark": false,
    "benchmark_description": "Enhanced version of popular MMLU dataset with questions across multiple subjects with higher difficulty tasks",
    "benchmark_paper": "https://github.com/TIGER-AI-Lab/MMLU-Pro"
  },
  {
    "benchmark_id": "multilingual-mmlu",
    "benchmark_name": "Multilingual MMLU",
    "benchmark_category": "General Intelligence",
    "featured_benchmark": false,
    "benchmark_description": "Evaluates models across 26 different languages and encompass three distinct tasks: ARC, HellaSwag, and MMLU",
    "benchmark_paper": "https://github.com/nlp-uoregon/mlmm-evaluation"
  },
  {
    "benchmark_id": "facts-grounding",
    "benchmark_name": "FACTS Grounding",
    "benchmark_category": "General Intelligence",
    "featured_benchmark": false,
    "benchmark_description": "Ability to provide factuality correct responses given documents and diverse user requests",
    "benchmark_paper": "https://storage.googleapis.com/deepmind-media/FACTS/FACTS_grounding_paper.pdf"
  },
  {
    "benchmark_id": "loft",
    "benchmark_name": "LOFT (128k)",
    "benchmark_category": "General Intelligence",
    "featured_benchmark": false,
    "benchmark_description": "Consists of 6 long-context task categories spanning retrieval, multi-hop compositional reasoning, and more",
    "benchmark_paper": "https://github.com/google-deepmind/loft"
  },
  {
    "benchmark_id": "mrcr",
    "benchmark_name": "MRCR (1M)",
    "benchmark_category": "General Intelligence",
    "featured_benchmark": false,
    "benchmark_description": "Novel, diagnostic long-context 71.9% understanding evaluation",
    "benchmark_paper": "https://arxiv.org/pdf/2409.12640v2"
  },
  {
    "benchmark_id": "simple-qa",
    "benchmark_name": "SimpleQA",
    "benchmark_category": "General Intelligence",
    "featured_benchmark": false,
    "benchmark_description": "World knowledge factuality with no search enabled",
    "benchmark_paper": "https://arxiv.org/abs/2411.04368"
  },
  {
    "benchmark_id": "aime-2024",
    "benchmark_name": "AIME 2024",
    "benchmark_category": "STEM",
    "featured_benchmark": false,
    "benchmark_description": "The series of exams used to challenge bright students on the path toward choosing the team that represents the United States at the International Mathematics Olympiad (IMO)",
    "benchmark_paper": "https://artofproblemsolving.com/wiki/index.php/AIME_Problems_and_Solutions"
  },
  {
    "benchmark_id": "aime-2025",
    "benchmark_name": "AIME 2025",
    "benchmark_category": "STEM",
    "featured_benchmark": false,
    "benchmark_description": "The series of exams used to challenge bright students on the path toward choosing the team that represents the United States at the International Mathematics Olympiad (IMO)",
    "benchmark_paper": "https://artofproblemsolving.com/wiki/index.php/AIME_Problems_and_Solutions"
  },
  {
    "benchmark_id": "gsm8k",
    "benchmark_name": "GSM8K",
    "benchmark_category": "STEM",
    "featured_benchmark": false,
    "benchmark_description": "A dataset of 8.5K high quality linguistically diverse grade school math word problems",
    "benchmark_paper": "https://arxiv.org/pdf/2110.14168"
  },
  {
    "benchmark_id": "hidden-math",
    "benchmark_name": "HiddenMath",
    "benchmark_category": "STEM",
    "featured_benchmark": false,
    "benchmark_description": "Competition-level math problems, Held out dataset AIME/AMC-like, crafted by experts and not leaked on the web",
    "benchmark_paper": null
  },
  {
    "benchmark_id": "math",
    "benchmark_name": "MATH",
    "benchmark_category": "STEM",
    "featured_benchmark": false,
    "benchmark_description": "Challenging math problems (incl. algebra, geometry, pre-calculus, and others)",
    "benchmark_paper": "https://arxiv.org/pdf/2103.03874"
  },
  {
    "benchmark_id": "math-500",
    "benchmark_name": "Math 500",
    "benchmark_category": "STEM",
    "featured_benchmark": false,
    "benchmark_description": "500 Challenging math problems (incl. algebra, geometry, pre-calculus, and others)",
    "benchmark_paper": "https://arxiv.org/pdf/2103.03874"
  },
  {
    "benchmark_id": "math-vista",
    "benchmark_name": "MathVista",
    "benchmark_category": "STEM",
    "featured_benchmark": false,
    "benchmark_description": "A benchmark designed to combine challenges from diverse mathematical and visual tasks",
    "benchmark_paper": "https://mathvista.github.io"
  },
  {
    "benchmark_id": "mgsm",
    "benchmark_name": "Mathematical Grade School Math",
    "benchmark_category": "STEM",
    "featured_benchmark": false,
    "benchmark_description": "Multilingual Grade School Math Benchmark",
    "benchmark_paper": "https://arxiv.org/pdf/2210.03057"
  },
  {
    "benchmark_id": "co-vo-st2",
    "benchmark_name": "CoVoST2",
    "benchmark_category": "General Intelligence",
    "featured_benchmark": false,
    "benchmark_description": "Automatic speech translation",
    "benchmark_paper": "https://github.com/facebookresearch/covost"
  },
  {
    "benchmark_id": "doc-vqa",
    "benchmark_name": "DocVQA",
    "benchmark_category": "General Intelligence",
    "featured_benchmark": false,
    "benchmark_description": "A series of challenges and release datasets to enable machines \"understand\" document images and thereby answer questions asked on them",
    "benchmark_paper": "https://www.docvqa.org"
  },
  {
    "benchmark_id": "ego-schema",
    "benchmark_name": "EgoSchema",
    "benchmark_category": "General Intelligence",
    "featured_benchmark": false,
    "benchmark_description": "Video analysis across multiple domains",
    "benchmark_paper": "https://arxiv.org/pdf/2308.09126"
  },
  {
    "benchmark_id": "mmmu",
    "benchmark_name": "MMMU",
    "benchmark_category": "General Intelligence",
    "featured_benchmark": true,
    "benchmark_description": "Multi-discipline college-level multimodal understanding and reasoning problems",
    "benchmark_paper": "https://mmmu-benchmark.github.io"
  },
  {
    "benchmark_id": "video-mme",
    "benchmark_name": "Video-MME",
    "benchmark_category": "General Intelligence",
    "featured_benchmark": false,
    "benchmark_description": "A full-spectrum Multi-Modal Evaluation benchmark of MLLMs in Video analysis",
    "benchmark_paper": "https://video-mme.github.io/home_page.html"
  },
  {
    "benchmark_id": "arc",
    "benchmark_name": "ARC",
    "benchmark_category": "reasoning",
    "featured_benchmark": false,
    "benchmark_description": "Used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans",
    "benchmark_paper": "https://arcprize.org/arc-agi"
  },
  {
    "benchmark_id": "charxiv-reasoning",
    "benchmark_name": "CharXiv-Reasoning",
    "benchmark_category": "reasoning",
    "featured_benchmark": false,
    "benchmark_description": "A comprehensive evaluation suite involving 2,323 natural, challenging, and diverse charts from scientific papers",
    "benchmark_paper": "https://charxiv.github.io"
  },
  {
    "benchmark_id": "drop",
    "benchmark_name": "DROP",
    "benchmark_category": "reasoning",
    "featured_benchmark": false,
    "benchmark_description": "A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs",
    "benchmark_paper": "https://arxiv.org/abs/1903.00161"
  },
  {
    "benchmark_id": "graphwalks-bfs",
    "benchmark_name": "Graphwalks BFS <128k accuracy",
    "benchmark_category": "reasoning",
    "featured_benchmark": false,
    "benchmark_description": "A dataset for evaluating multi-hop long-context reasoning",
    "benchmark_paper": "https://huggingface.co/datasets/openai/graphwalks"
  },
  {
    "benchmark_id": "hellaswag",
    "benchmark_name": "Hellaswag",
    "benchmark_category": "reasoning",
    "featured_benchmark": false,
    "benchmark_description": "Can a Machine Really Finish Your Sentence?",
    "benchmark_paper": "https://arxiv.org/abs/1905.07830"
  },
  {
    "benchmark_id": "humanitys-last-exam",
    "benchmark_name": "Humanity's Last Exam",
    "benchmark_category": "reasoning",
    "featured_benchmark": false,
    "benchmark_description": "A multi-modal benchmark at the frontier of human knowledge, designed to be the final closed-ended academic benchmark of its kind with broad subject coverage",
    "benchmark_paper": "https://agi.safe.ai"
  },
  {
    "benchmark_id": "simple-bench",
    "benchmark_name": "SimpleBench",
    "benchmark_category": "reasoning",
    "featured_benchmark": true,
    "benchmark_description": "A multiple-choice text benchmark for LLMs where individuals with unspecialized (high school) knowledge outperform SOTA models.",
    "benchmark_paper": "https://simple-bench.com"
  },
  {
    "benchmark_id": "ai2d",
    "benchmark_name": "AI2D",
    "benchmark_category": "STEM",
    "featured_benchmark": false,
    "benchmark_description": "A dataset of over 5,000 grade school science diagrams with over 150,000 rich annotations",
    "benchmark_paper": "https://paperswithcode.com/dataset/ai2d"
  },
  {
    "benchmark_id": "gpqa-diamond",
    "benchmark_name": "GPQA Diamond",
    "benchmark_category": "STEM",
    "featured_benchmark": true,
    "benchmark_description": "Challenging dataset of questions written by domain experts in biology, physics, and chemistry",
    "benchmark_paper": "https://arxiv.org/abs/2311.12022"
  }
]